# Incidente Amazon Web Services (AWS) â€” 20 de octubre de 2025

## 1) LÃ­nea de tiempo

### ðŸ•› 11:49 PM PDT (19 oct)

AWS comienza a observar **errores y latencias** en mÃºltiples servicios.

> _AÃºn sin identificar la causa raÃ­z._

### ðŸ• 12:51 AM PDT

Confirman que **mÃºltiples servicios presentan fallos de API y soporte tÃ©cnico (Support Center)**.

> Indica que el impacto llega hasta el plano administrativo.

### ðŸ•œ 1:26 AM PDT

Se confirma que el problema principal es con **DynamoDB API endpoint US-EAST-1**.

> El fallo en DynamoDB afecta a servicios que dependen de Ã©l (IAM, S3 control plane, Lambda).

### ðŸ•‘ 2:01 AM PDT

Identifican el **posible origen**: **resoluciÃ³n DNS del endpoint de DynamoDB**.

> Se inician mitigaciones en paralelo.  
> La degradaciÃ³n de DNS interno provoca â€œtimeoutsâ€ en toda la malla de servicios.

### ðŸ•“ 2:22 AM â€“ 2:27 AM PDT

Aplican mitigaciones iniciales y comienzan los **primeros signos de recuperaciÃ³n**.

> Muchos requests aÃºn fallan porque los clientes mantienen cachÃ© DNS corrupta.

### ðŸ•” 3:03 AM â€“ 3:35 AM PDT

El **problema DNS se resuelve completamente**.

> La mayorÃ­a de servicios vuelve a funcionar.  
> Persisten errores en **EC2 instance launch** y **ECS** por dependencias de DynamoDB.  
> AWS recomienda **limpiar cachÃ©s DNS locales**.

### ðŸ•• 4:08 AM â€“ 4:48 AM PDT

El foco se traslada a **EC2**, donde un subsistema interno no logra lanzar nuevas instancias.

> EC2, RDS, ECS y Glue dependen de este plano de lanzamiento.  
> TambiÃ©n se observan **demoras en Lambda y SQS**.  
> AWS limita (â€œrate limitâ€) los lanzamientos para estabilizar.

### ðŸ•– 5:10 AM â€“ 5:48 AM PDT

-   Se **recupera el procesamiento de colas SQS vÃ­a Lambda**.
-   EC2 lanza instancias en algunas AZ, con progresos graduales.
    > _Primera evidencia de recuperaciÃ³n estructural._

### ðŸ•— 6:42 AM â€“ 7:29 AM PDT

-   AÃºn existen **errores elevados en lanzamientos EC2**.
-   Se detectan **problemas de red internos (connectivity issues)**.
    > AquÃ­ el evento deja de ser de datos (Dynamo) y pasa a infraestructura (red interna).

### ðŸ•˜ 8:04 AM â€“ 8:43 AM PDT

-   AWS confirma que la falla de red proviene del **subsystem interno de Network Load Balancer (NLB)**.
-   Se **restringen nuevos lanzamientos EC2** para ayudar a la recuperaciÃ³n.
    > Este es el segundo â€œtriggerâ€ del evento, posterior al DNS.

### ðŸ•™ 9:13 AM â€“ 10:38 AM PDT

-   Se aplican mitigaciones en el subsistema NLB.
-   Conectividad y APIs comienzan a estabilizarse.
-   **EC2** muestra signos de recuperaciÃ³n en varias AZ.
    > Los equipos de AWS priorizan zonas con mayor densidad de clientes.

### ðŸ•› 11:22 AM â€“ 12:15 PM PDT

-   Aumenta la tasa de lanzamientos EC2 exitosos.
-   **Lambda** mejora sus invocaciones, aunque algunas funciones aÃºn fallan.
-   Se **reduce el throttling** y se normaliza el polling de SQS.
    > Empieza la etapa de â€œservice-by-service recoveryâ€.

### ðŸ• 1:03 PM â€“ 1:52 PM PDT

-   Todos los servicios mejoran.
-   Lambda se normaliza.
-   EC2 lanza instancias en todas las AZ, pero con throttles parciales.
    > El backlog de colas SQS y Lambda se procesa lentamente.

### ðŸ•’ 2:48 PM â€“ 3:01 PM PDT (RESOLVED)

-   Se eliminan todos los throttles.
-   EC2 y Redshift recuperan 100 %.
-   AWS Connect funciona normalmente.
-   Queda backlog en Config, Redshift y Connect que se procesa en horas siguientes.

## 2) VersiÃ³n oficial (AWS)

-   AWS publica en su blog que â€œentre 11:49 PM PDT del 19 de octubre y 2:24 AM PDT del 20 de octubre, los servicios en US-EAST-1 experimentaron tasas elevadas de error, debido a problemas de resoluciÃ³n DNS para endpoints de DynamoDBâ€. Fuente: [AboutAmazon (AWS blog)](https://www.aboutamazon.com/news/aws/aws-service-disruptions-outage-update)
-   En el panel de estado (â€œAWS Health Dashboardâ€) se muestra el evento: â€œMultiple AWS services experienced network connectivity issues in the US-EAST-1 Regionâ€. Fuente: [AWS Health Dashboard](https://health.aws.amazon.com/health/status?ts=20251020)
    ![Diagrama](./_img/1.png)

## 3) VersiÃ³n no oficial (prensa / analistas)

-   El fallo pone de manifiesto la vulnerabilidad de depender en gran medida de un solo proveedor de nube. Fuente: [The Guardian](https://www.theguardian.com/technology/2025/oct/20/amazon-web-services-aws-outage-hits-dozens-websites-apps)
-   El problema tÃ©cnico principal identificado: error en el DNS interno, lo cual generÃ³ fallos en cadena. Fuente: [Business Insider](https://www.businessinsider.com/dns-error-major-aws-outage-amazon-snapchat-reddit-venmo-2025-10)
-   ArtÃ­culo tÃ©cnico que profundiza en por quÃ© una degradaciÃ³n DNS se propagÃ³ a gran escala. Fuente: [Wired](https://www.wired.com/story/what-that-huge-aws-outage-reveals-about-the-internet/)
    ![Diagrama](./_img/2.webp)

## 4) Impacto en el ecosistema

-   Se vieron afectados grandes plataformas: juegos, apps de mensajerÃ­a, servicios financieros, dispositivos IoT (como cÃ¡maras de seguridad) y mÃ¡s. Fuente: [TechRadar](https://www.techradar.com/news/live/amazon-web-services-alexa-ring-snapchat-fortnite-down-october-2025)
-   Sectores crÃ­ticos (banca, salud, gobierno) reportaron interrupciones, evidenciando el alcance de la dependencia en la nube. Fuente: [eMarketer](https://www.emarketer.com/content/aws-outage-exposes-growing-fragility-cloud-economy)
    ![Diagrama](./_img/3.avif)

## 5) Lecciones tÃ©cnicas para arquitecturas cloud

1. El servicio DNS **no es trivial**: una falla allÃ­ puede cortar el descubrimiento de endpoints y provocar reintentos masivos.
2. Blast radius de la regiÃ³n US-EAST-1: cuando tus dependencias (control plane, datos crÃ­ticos) estÃ¡n centralizadas allÃ­, el impacto puede ser severo.
3. DiseÃ±ar para backlog y colas: despuÃ©s de la interrupciÃ³n, muchos servicios tuvieron que procesar colas de mensajes acumulados.
4. Separar plano de control vs plano de datos, y evitar que un fallo de â€œmanagement endpointâ€ afecte el trÃ¡fico de cliente.
   ![Diagrama](./_img/5.png)
   ![Diagrama](./_img/4.png)

## 6) Â¿Por quÃ© muchas arquitecturas hÃ­bridas/multi-cloud no mitigaron el impacto?

-   Tener web/frontend en otra nube no basta si los datos o identidad siguen en AWS y se ven afectados.
-   ReplicaciÃ³n de datos entre nubes muchas veces es **pasiva**, con RTO/RPO que no sirven ante un fallo activo de control plane.
-   Operativa, observabilidad, secretos y colas rara vez estÃ¡n orquestados y probados en mÃºltiples nubes.
-   Coste y complejidad: muchas empresas dicen â€œmulti-cloudâ€ pero en realidad tienen **activo-pasivo teÃ³rico**, sin drills de fail-over reales.
-   Dependencias cruzadas externas: aunque tu mÃ³dulo estÃ© en otra nube, si el proveedor de notificaciones, auth o pagos estÃ¡ en AWS, el fallo les llega igual.
    ![Diagrama](./_img/6.svg)

## 7) Checklist de resiliencia para cargas en AWS

-   Tener **multi-regiÃ³n activa**: mÃ­nimo una regiÃ³n secundaria con datos sincronizados, no solo standby.
-   Revisar todos los puntos de dependencia (DNS interno, control plane, colas, IAM) y asegurar fallback.
-   TTL de DNS razonables y cachÃ©s de endpoints crÃ­ticos con opciÃ³n de fallback.
-   DiseÃ±ar â€œmodo degradadoâ€: por ejemplo, lectura en sÃ³lo lectura, colas locales, fallback limitado.
-   Realizar **DR drills regulares**: poner a prueba failover, rollback, cargas y latencias reales.
-   Monitorizar â€œdependencias de proveedorâ€ para servicios de terceros hospedados en la nube que puedan fallar.
-   Incluir en los SLAs internos la posibilidad de degradaciÃ³n severa de un proveedor nube y el impacto en negocio.

---

### Preguntas para discusiÃ³n en clase

1. Â¿CuÃ¡les son los riesgos de tener toda tu infraestructura crÃ­tica en una sola regiÃ³n de nube?
2. En una arquitectura hÃ­brida (on-premises + nube) Â¿cÃ³mo garantizarÃ­as que un fallo en la nube no afecte los procesos on-premises esenciales?
3. Â¿QuÃ© significa hoy â€œmulti-cloud Ãºtilâ€ vs â€œmulti-cloud de marketingâ€?
4. Â¿QuÃ© mÃ©tricas de negocio (SLO, SLA) deberÃ­an revisarse tras un incidente de esta magnitud?
5. Â¿CÃ³mo diseÃ±arÃ­as un plan de recuperaciÃ³n post-incidente para minimizar el backlog y volver a operaciÃ³n normal mÃ¡s rÃ¡pido?

---

> **Fuentes clave:**
>
> -   [The Verge: Major AWS outage took down Fortnite, Alexa, Snapchat, and more](https://www.theverge.com/news/802486/aws-outage-alexa-fortnite-snapchat-offline)
> -   [The Guardian: Amazon Web Services outage shows internet users 'at mercy' of too few providers](https://www.theguardian.com/technology/2025/oct/20/amazon-web-services-aws-outage-hits-dozens-websites-apps)
> -   [Reuters: Amazon says AWS cloud service back to normal after outage disrupts businesses worldwide](https://www.reuters.com/business/retail-consumer/amazons-cloud-unit-reports-outage-several-websites-down-2025-10-20/)
> -   [AWS blog: Update â€“ AWS services operating normally](https://www.aboutamazon.com/news/aws/aws-service-disruptions-outage-update)
